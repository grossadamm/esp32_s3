<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>üéôÔ∏è Voice Agent</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 20px;
            color: white;
        }

        .container {
            max-width: 400px;
            width: 100%;
            text-align: center;
        }

        .title {
            font-size: 2.5rem;
            margin-bottom: 10px;
            font-weight: 300;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.8;
            margin-bottom: 40px;
        }

        .status {
            font-size: 1.2rem;
            margin-bottom: 30px;
            padding: 12px 20px;
            border-radius: 25px;
            font-weight: 500;
            transition: all 0.3s ease;
        }

        .status.disconnected {
            background: rgba(255, 255, 255, 0.1);
            border: 2px solid rgba(255, 255, 255, 0.3);
        }

        .status.connected {
            background: rgba(76, 175, 80, 0.2);
            border: 2px solid #4CAF50;
            color: #4CAF50;
        }

        .status.recording {
            background: rgba(244, 67, 54, 0.2);
            border: 2px solid #F44336;
            color: #F44336;
            animation: pulse 1.5s infinite;
        }

        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }

        .button {
            width: 100%;
            padding: 20px;
            margin: 15px 0;
            border: none;
            border-radius: 50px;
            font-size: 1.3rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            background: rgba(255, 255, 255, 0.1);
            color: white;
            border: 2px solid rgba(255, 255, 255, 0.3);
            min-height: 60px;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
        }

        .button:hover {
            background: rgba(255, 255, 255, 0.2);
            transform: translateY(-2px);
        }

        .button:active {
            transform: translateY(0);
            background: rgba(255, 255, 255, 0.3);
        }

        .button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .button.primary {
            background: rgba(76, 175, 80, 0.2);
            border-color: #4CAF50;
            color: #4CAF50;
        }

        .button.danger {
            background: rgba(244, 67, 54, 0.2);
            border-color: #F44336;
            color: #F44336;
        }

        .button.recording {
            background: #F44336;
            border-color: #F44336;
            color: white;
            animation: pulse 1.5s infinite;
        }

        .button.permission {
            background: rgba(255, 193, 7, 0.2);
            border-color: #FFC107;
            color: #FFC107;
            animation: glow 2s ease-in-out infinite alternate;
        }

        @keyframes glow {
            from { box-shadow: 0 0 5px rgba(255, 193, 7, 0.2); }
            to { box-shadow: 0 0 20px rgba(255, 193, 7, 0.4); }
        }

        .transcription {
            margin-top: 30px;
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            min-height: 60px;
            font-size: 1.1rem;
            line-height: 1.4;
            opacity: 0.9;
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .transcription.empty {
            opacity: 0.5;
            font-style: italic;
        }

        .version {
            position: fixed;
            bottom: 10px;
            right: 10px;
            font-size: 0.8rem;
            opacity: 0.5;
        }

        /* Responsive adjustments */
        @media (max-width: 480px) {
            .title {
                font-size: 2rem;
            }
            
            .button {
                font-size: 1.2rem;
                padding: 18px;
            }
            
            .container {
                padding: 0 10px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="title">üéôÔ∏è Voice Agent</h1>
        <p class="subtitle">Tap to talk with your AI assistant</p>
        
        <div id="status" class="status disconnected">Disconnected</div>
        
        <button id="connectBtn" class="button primary">
            <span>üîå</span> Connect
        </button>
        
        <button id="permissionBtn" class="button permission" style="display: none;">
            <span>üîí</span> <span>Allow Microphone Access</span>
        </button>
        
        <button id="recordBtn" class="button" disabled>
            <span>üé§</span> <span id="recordText">Hold to Talk</span>
        </button>
        
        <button id="disconnectBtn" class="button danger" disabled>
            <span>‚ùå</span> Disconnect
        </button>
        
        <button id="testAudioBtn" class="button" style="display: none;">
            <span>üîä</span> Test Audio
        </button>
        
        <div id="transcription" class="transcription empty">
            Your conversation will appear here...
        </div>
    </div>

    <div class="version">v1.0</div>

    <script>
        let ws = null;
        let isRecording = false;
        let isConnected = false;
        let microphonePermissionGranted = false;

        const connectBtn = document.getElementById('connectBtn');
        const permissionBtn = document.getElementById('permissionBtn');
        const recordBtn = document.getElementById('recordBtn');
        const disconnectBtn = document.getElementById('disconnectBtn');
        const testAudioBtn = document.getElementById('testAudioBtn');
        const status = document.getElementById('status');
        const transcription = document.getElementById('transcription');
        const recordText = document.getElementById('recordText');

        // Check if we're on a secure connection (required for microphone access)
        function checkSecureConnection() {
            const hostname = window.location.hostname;
            const isLocalHost = hostname === 'localhost' || hostname === '127.0.0.1';
            const isLocalNetwork = hostname.startsWith('192.168.') || hostname.startsWith('10.') || hostname.startsWith('172.');
            const isSecure = window.location.protocol === 'https:';
            
            // Allow localhost, local network IPs, or HTTPS
            if (!isSecure && !isLocalHost && !isLocalNetwork) {
                updateStatus('HTTPS Required', 'disconnected');
                updateTranscription('üîí Microphone access requires a secure connection (HTTPS) for external domains. Please use HTTPS or access via local network.', true);
                return false;
            }
            return true;
        }

        // WebSocket connection
        async function connect() {
            if (!checkSecureConnection()) {
                return;
            }

            // Initialize audio context (requires user gesture on iOS)
            console.log('Initializing audio context...');
            await initializeAudio();

            // Check microphone permission first
            await checkInitialPermissions();
            
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsUrl = `${protocol}//${window.location.host}/api/audio/realtime`;
            
            ws = new WebSocket(wsUrl);
            
            ws.onopen = () => {
                console.log('Connected to voice agent');
                isConnected = true;
                updateUI();
                updateStatus('Connected', 'connected');
            };
            
            ws.onmessage = (event) => {
                try {
                    const data = JSON.parse(event.data);
                    handleWebSocketMessage(data);
                } catch (error) {
                    console.error('Error parsing WebSocket message:', error);
                }
            };
            
            ws.onclose = () => {
                console.log('Disconnected from voice agent');
                isConnected = false;
                isRecording = false;
                updateUI();
                updateStatus('Disconnected', 'disconnected');
            };
            
            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
                updateStatus('Connection Error', 'disconnected');
            };
        }

        function disconnect() {
            if (ws) {
                ws.close();
            }
            stopRecording();
            
            // Stop any currently playing audio
            stopAllAudio();
        }

        function handleWebSocketMessage(data) {
            console.log('WebSocket message:', data.type, data);
            
            switch (data.type) {
                case 'session_ready':
                    console.log('Session ready');
                    break;
                case 'speech_started':
                    updateStatus('Listening...', 'recording');
                    break;
                case 'speech_stopped':
                    updateStatus('Processing...', 'connected');
                    break;
                case 'transcript_delta':
                    console.log('Transcript:', data.text);
                    updateTranscription(data.text, false);
                    break;
                case 'audio_delta':
                    console.log('Audio delta received, audio length:', data.audio ? data.audio.length : 'none');
                    if (data.audio) {
                        playAudioChunk(data.audio);
                        updateStatus('üîä Playing response...', 'connected');
                    }
                    break;
                case 'error':
                    console.error('Server error:', data.message);
                    updateStatus('Error: ' + data.message, 'disconnected');
                    break;
                default:
                    console.log('Unknown message type:', data.type);
            }
        }

        function updateTranscription(text, isComplete = true) {
            if (text && text.trim()) {
                transcription.textContent = text;
                transcription.classList.remove('empty');
            } else if (!text) {
                transcription.textContent = 'Your conversation will appear here...';
                transcription.classList.add('empty');
            }
        }

        let audioContext = null;
        let currentAudioSource = null;
        let isPlayingAudio = false;
        
        // Audio queue for seamless chunk playback
        let audioQueue = [];
        let isProcessingQueue = false;
        let nextStartTime = 0;

        // Initialize audio context (requires user interaction on iOS)
        async function initializeAudio() {
            if (!audioContext) {
                try {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)({
                        sampleRate: 24000  // Match OpenAI's output
                    });
                    console.log('Audio context initialized:', audioContext.state);
                    
                    // Resume audio context if suspended (iOS requirement)
                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                        console.log('Audio context resumed');
                    }
                    return true;
                } catch (error) {
                    console.error('Failed to initialize audio context:', error);
                    return false;
                }
            } else {
                // Always try to resume on user interaction
                if (audioContext.state === 'suspended') {
                    try {
                        await audioContext.resume();
                        console.log('Audio context resumed on user interaction');
                    } catch (error) {
                        console.error('Failed to resume audio context:', error);
                    }
                }
            }
            return true;
        }

        async function playAudioChunk(base64Audio) {
            console.log('Received audio chunk, length:', base64Audio.length);
            
            try {
                if (!audioContext) {
                    console.error('Audio context not initialized');
                    updateStatus('üîä Audio received (no context)', 'connected');
                    return;
                }

                // Resume audio context if suspended (iOS requirement)
                if (audioContext.state === 'suspended') {
                    console.log('Resuming audio context...');
                    await audioContext.resume();
                }

                // Convert base64 to AudioBuffer
                const audioBuffer = await decodeAudioChunk(base64Audio);
                if (!audioBuffer) return;

                // Add to queue for seamless playback
                audioQueue.push(audioBuffer);
                
                // Start processing queue if not already processing
                if (!isProcessingQueue) {
                    processAudioQueue();
                }
                
                updateStatus('üîä Playing response...', 'connected');
                
            } catch (error) {
                console.error('Error processing audio chunk:', error);
                updateStatus('üîä Audio received (process failed)', 'connected');
            }
        }

        async function decodeAudioChunk(base64Audio) {
            try {
                // Decode base64 to binary (raw PCM16 data from OpenAI)
                const binaryString = atob(base64Audio);
                const arrayBuffer = new ArrayBuffer(binaryString.length);
                const uint8Array = new Uint8Array(arrayBuffer);
                
                for (let i = 0; i < binaryString.length; i++) {
                    uint8Array[i] = binaryString.charCodeAt(i);
                }

                // Convert raw PCM16 data to AudioBuffer manually
                // OpenAI sends PCM16 at 24kHz mono
                const pcmData = new Int16Array(arrayBuffer);
                const sampleRate = 24000;
                const numSamples = pcmData.length;
                const duration = numSamples / sampleRate;
                
                console.log(`Audio chunk: ${numSamples} samples, ${duration.toFixed(3)}s duration`);
                
                // Create AudioBuffer manually
                const audioBuffer = audioContext.createBuffer(1, numSamples, sampleRate);
                const channelData = audioBuffer.getChannelData(0);
                
                // Convert Int16 PCM to Float32 audio data
                for (let i = 0; i < numSamples; i++) {
                    channelData[i] = pcmData[i] / 32768.0; // Convert from -32768,32767 to -1,1
                }
                
                return audioBuffer;
                
            } catch (error) {
                console.error('Error decoding audio chunk:', error);
                return null;
            }
        }

        function processAudioQueue() {
            if (isProcessingQueue || audioQueue.length === 0) {
                return;
            }
            
            isProcessingQueue = true;
            isPlayingAudio = true;
            
            // Initialize start time if this is the first chunk
            if (nextStartTime === 0) {
                nextStartTime = audioContext.currentTime + 0.1; // Small delay to prevent clicks
            }
            
            console.log(`Processing audio queue: ${audioQueue.length} chunks`);
            
            // Process all queued chunks
            while (audioQueue.length > 0) {
                const audioBuffer = audioQueue.shift();
                
                // Create buffer source
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                
                // Schedule to play at the exact time the previous chunk ends
                source.start(nextStartTime);
                
                // Update next start time for seamless playback
                nextStartTime += audioBuffer.duration;
                
                console.log(`Scheduled chunk at ${nextStartTime - audioBuffer.duration}s, duration: ${audioBuffer.duration.toFixed(3)}s`);
                
                // Handle the last chunk ending
                if (audioQueue.length === 0) {
                    source.onended = () => {
                        console.log('Audio queue playback completed');
                        isProcessingQueue = false;
                        isPlayingAudio = false;
                        nextStartTime = 0;
                        updateStatus('Connected', 'connected');
                    };
                }
            }
        }

        function stopAllAudio() {
            // Clear the queue and reset state
            audioQueue = [];
            isProcessingQueue = false;
            isPlayingAudio = false;
            nextStartTime = 0;
            
            // Stop current audio source if exists
            if (currentAudioSource) {
                try {
                    currentAudioSource.stop();
                    currentAudioSource.disconnect();
                    currentAudioSource = null;
                    console.log('Stopped current audio source');
                } catch (e) {
                    // Audio source may already be stopped
                }
            }
            
            console.log('Stopped all audio and cleared queue');
        }



        async function checkMicrophonePermission() {
            try {
                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    throw new Error('Microphone not supported');
                }

                // Check permission status
                if (navigator.permissions) {
                    const permission = await navigator.permissions.query({ name: 'microphone' });
                    return permission.state;
                }
                
                return 'unknown';
            } catch (error) {
                console.error('Permission check error:', error);
                return 'unknown';
            }
        }

        async function checkInitialPermissions() {
            const permissionState = await checkMicrophonePermission();
            
            if (permissionState === 'granted') {
                microphonePermissionGranted = true;
                updateUI();
            } else if (permissionState === 'denied') {
                updateStatus('Permission Denied', 'disconnected');
                updateTranscription('üîí Microphone access was previously denied. Please enable microphone permissions in your browser settings and refresh the page.', true);
            } else {
                // Permission is 'prompt' or 'unknown' - show permission button
                permissionBtn.style.display = 'block';
                updateStatus('Permission Required', 'disconnected');
                updateTranscription('üé§ Microphone access is required for voice chat. Click "Allow Microphone Access" and then "Allow" in the browser prompt.', true);
            }
        }

        async function requestMicrophonePermission() {
            try {
                updateStatus('Requesting Permission...', 'connected');
                
                // Request permission by attempting to get user media
                // This MUST be called from a user gesture (button click)
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 24000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });
                
                // Stop the stream immediately - we just wanted to check permission
                stream.getTracks().forEach(track => track.stop());
                
                microphonePermissionGranted = true;
                permissionBtn.style.display = 'none';
                updateStatus('Connected', 'connected');
                updateTranscription('‚úÖ Microphone access granted! You can now hold the button to talk.', true);
                updateUI();
                
                return true;
            } catch (error) {
                console.error('Permission request error:', error);
                handleMicrophoneError(error);
                return false;
            }
        }

        function handleMicrophoneError(error) {
            let errorMessage = 'Microphone Error';
            
            if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                errorMessage = 'Microphone Permission Denied';
            } else if (error.name === 'NotFoundError') {
                errorMessage = 'No Microphone Found';
            } else if (error.name === 'NotSupportedError') {
                errorMessage = 'Microphone Not Supported';
            } else if (error.name === 'NotReadableError') {
                errorMessage = 'Microphone Already in Use';
            }
            
            updateStatus(errorMessage, 'disconnected');
            
            // Show user-friendly guidance
            if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                updateTranscription('üîí Please allow microphone access in your browser settings and refresh the page.', true);
            }
        }

        let audioWorkletNode = null;
        let recordingStream = null;

        async function startRecording() {
            try {
                // Stop any currently playing audio before recording
                stopAllAudio();
                
                // Ensure audio context is ready for playback (iOS requirement)
                await initializeAudio();
                
                // Check permission first
                const permissionState = await checkMicrophonePermission();
                
                if (permissionState === 'denied') {
                    updateStatus('Permission Denied', 'disconnected');
                    updateTranscription('üîí Microphone access was denied. Please enable microphone permissions in your browser settings and refresh the page.', true);
                    return;
                }

                // Get audio stream with proper format for OpenAI Realtime API
                recordingStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 24000,  // OpenAI requires 24kHz
                        channelCount: 1,    // Mono
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });

                // Use Web Audio API to get raw PCM data
                const audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 24000 });
                const source = audioCtx.createMediaStreamSource(recordingStream);
                
                // Create a script processor to capture raw audio data
                const processor = audioCtx.createScriptProcessor(2048, 1, 1);
                
                processor.onaudioprocess = (event) => {
                    if (ws && ws.readyState === WebSocket.OPEN && isRecording) {
                        const inputBuffer = event.inputBuffer;
                        const inputData = inputBuffer.getChannelData(0); // Get mono channel
                        
                        // Convert float32 to int16 (PCM16)
                        const pcmData = new Int16Array(inputData.length);
                        for (let i = 0; i < inputData.length; i++) {
                            // Convert from -1,1 float to -32768,32767 int16
                            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32767));
                        }
                        
                        // Convert to base64
                        const uint8Array = new Uint8Array(pcmData.buffer);
                        const base64Audio = btoa(String.fromCharCode.apply(null, uint8Array));
                        
                        // Send to server
                        ws.send(JSON.stringify({
                            type: 'audio_chunk',
                            audio: base64Audio
                        }));
                    }
                };
                
                source.connect(processor);
                processor.connect(audioCtx.destination);
                
                audioWorkletNode = processor;
                isRecording = true;
                updateStatus('Recording...', 'recording');
                updateUI();
                
                console.log('Started recording with PCM16 at 24kHz');
                
            } catch (error) {
                console.error('Error starting recording:', error);
                handleMicrophoneError(error);
            }
        }

        function stopRecording() {
            if (isRecording) {
                isRecording = false;
                
                // Clean up Web Audio API resources
                if (audioWorkletNode) {
                    audioWorkletNode.disconnect();
                    audioWorkletNode = null;
                }
                
                // Stop the recording stream
                if (recordingStream) {
                    recordingStream.getTracks().forEach(track => track.stop());
                    recordingStream = null;
                }
                
                updateStatus('Connected', 'connected');
                updateUI();
                
                console.log('Stopped recording');
            }
        }

        function updateStatus(text, className) {
            status.textContent = text;
            status.className = `status ${className}`;
        }

        async function testAudio() {
            console.log('Testing audio playback...');
            
            try {
                // Ensure audio context is ready
                await initializeAudio();
                
                if (!audioContext) {
                    updateStatus('Audio context not ready', 'disconnected');
                    return;
                }

                // Resume if suspended
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Stop any currently playing audio before test
                stopAllAudio();

                const oscillator = audioContext.createOscillator();
                const gainNode = audioContext.createGain();
                
                oscillator.connect(gainNode);
                gainNode.connect(audioContext.destination);
                
                oscillator.frequency.value = 440; // A4 note
                gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
                gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.5);
                
                oscillator.start(audioContext.currentTime);
                oscillator.stop(audioContext.currentTime + 0.5);
                
                updateStatus('üîä Test tone playing', 'connected');
                setTimeout(() => updateStatus('Connected', 'connected'), 1000);
                
                console.log('Test audio played successfully');
            } catch (error) {
                console.error('Test audio error:', error);
                updateStatus('Audio test failed', 'disconnected');
            }
        }

        function updateUI() {
            connectBtn.disabled = isConnected;
            disconnectBtn.disabled = !isConnected;
            recordBtn.disabled = !isConnected || !microphonePermissionGranted;
            
            // Show test audio button when connected
            testAudioBtn.style.display = isConnected ? 'block' : 'none';
            
            // Show/hide permission button based on state
            if (!microphonePermissionGranted && isConnected) {
                permissionBtn.style.display = 'block';
            } else {
                permissionBtn.style.display = 'none';
            }
            
            if (isRecording) {
                recordBtn.classList.add('recording');
                recordText.textContent = 'Recording...';
            } else {
                recordBtn.classList.remove('recording');
                recordText.textContent = microphonePermissionGranted ? 'Hold to Talk' : 'Permission Required';
            }
        }

        // Event listeners
        connectBtn.addEventListener('click', connect);
        disconnectBtn.addEventListener('click', disconnect);
        permissionBtn.addEventListener('click', requestMicrophonePermission);
        testAudioBtn.addEventListener('click', testAudio);

        // Touch-friendly recording (hold to talk)
        recordBtn.addEventListener('touchstart', (e) => {
            e.preventDefault();
            if (!isRecording && isConnected) {
                startRecording();
            }
        });

        recordBtn.addEventListener('touchend', (e) => {
            e.preventDefault();
            if (isRecording) {
                stopRecording();
            }
        });

        // Mouse events for desktop testing
        recordBtn.addEventListener('mousedown', (e) => {
            if (!isRecording && isConnected) {
                startRecording();
            }
        });

        recordBtn.addEventListener('mouseup', (e) => {
            if (isRecording) {
                stopRecording();
            }
        });

        // Prevent context menu on long press
        recordBtn.addEventListener('contextmenu', (e) => {
            e.preventDefault();
        });

        // Initialize the app
        async function initialize() {
            if (!checkSecureConnection()) {
                return;
            }
            
            // Debug browser capabilities
            console.log('Browser check:', {
                hasNavigator: !!navigator,
                hasMediaDevices: !!navigator.mediaDevices,
                hasGetUserMedia: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia),
                userAgent: navigator.userAgent,
                protocol: window.location.protocol,
                hostname: window.location.hostname
            });
            
            // Check if browser supports required APIs
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                updateStatus('Not Supported', 'disconnected');
                updateTranscription(`‚ùå Browser API check failed. Debug info: navigator.mediaDevices=${!!navigator.mediaDevices}, getUserMedia=${!!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia)}. Please ensure you're using a secure connection.`, true);
                return;
            }
            
            updateUI();
            
            // Check permissions on load (but don't show permission button until connected)
            const permissionState = await checkMicrophonePermission();
            if (permissionState === 'granted') {
                microphonePermissionGranted = true;
                updateUI();
            }
        }

        // Initialize UI
        initialize();
    </script>
</body>
</html> 