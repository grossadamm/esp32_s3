<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Realtime Audio Test Client</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .status {
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
        }
        .connected { background-color: #d4edda; color: #155724; }
        .disconnected { background-color: #f8d7da; color: #721c24; }
        .recording { background-color: #fff3cd; color: #856404; }
        .audio-ready { background-color: #cce5ff; color: #004085; }
        .vad-ready { background-color: #e7f3ff; color: #004085; }
        .speaking { background-color: #d1ecf1; color: #0c5460; }
        button {
            padding: 10px 20px;
            margin: 5px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        .start { background-color: #28a745; color: white; }
        .stop { background-color: #dc3545; color: white; }
        .transcript {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 5px;
            padding: 15px;
            margin: 10px 0;
            min-height: 100px;
            white-space: pre-wrap;
        }
        .log {
            background-color: #f1f3f4;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            max-height: 300px;
            overflow-y: auto;
            font-family: monospace;
            font-size: 12px;
        }
    </style>
    <!-- VAD Library (ricky0123/vad-web) -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.22/dist/bundle.min.js"></script>
</head>
<body>
    <h1>üéôÔ∏è Realtime Audio Test Client</h1>
    
    <div id="status" class="status disconnected">
        Disconnected
    </div>
    
    <div>
        <button id="connectBtn" class="start" onclick="connect()">Connect</button>
        <button id="disconnectBtn" class="stop" onclick="disconnect()" disabled>Disconnect</button>
        <button id="recordBtn" class="start" onclick="startRecording()" disabled>Start Recording</button>
        <button id="stopBtn" class="stop" onclick="stopRecording()" disabled>Stop Recording</button>
    </div>
    
    <h3>Live Transcript:</h3>
    <div id="transcript" class="transcript"></div>
    
    <h3>Connection Log:</h3>
    <div id="log" class="log"></div>

    <script>
        let ws = null;
        let mediaRecorder = null;
        let audioContext = null;
        let silentAudio = null;
        let audioContextReady = false;
        let audioQueue = [];
        let vad = null;
        let vadReady = false;
        let isSpeaking = false;
        
        // üéØ AUDIO CONTEXT KEEPALIVE MANAGER
        class AudioContextKeepAlive {
            constructor() {
                this.audioContext = null;
                this.silentAudio = null;
                this.isActivated = false;
                this.monitorInterval = null;
            }
            
            async initializeOnUserGesture() {
                if (this.isActivated) return;
                
                try {
                    log('üéß Initializing AudioContext keepalive...');
                    
                    // 1. Create AudioContext
                    this.audioContext = new AudioContext();
                    
                    // 2. Resume if suspended (browser policy compliance)
                    if (this.audioContext.state === 'suspended') {
                        await this.audioContext.resume();
                        log('‚ñ∂Ô∏è AudioContext resumed');
                    }
                    
                    // 3. Create silent audio keepalive (cleaner approach)
                    this.createSilentAudioKeepalive();
                    
                    // 4. Set up periodic monitoring
                    this.startMonitoring();
                    
                    this.isActivated = true;
                    audioContextReady = true;
                    
                    log('‚úÖ AudioContext keepalive activated!');
                    updateStatus('Audio System Ready', 'audio-ready');
                    
                } catch (error) {
                    log(`‚ùå AudioContext init failed: ${error.message}`);
                }
            }
            
            createSilentAudioKeepalive() {
                // Create a silent audio source using AudioContext
                const sampleRate = this.audioContext.sampleRate;
                const buffer = this.audioContext.createBuffer(1, 1, sampleRate);
                
                // Create silent buffer source
                const silentSource = this.audioContext.createBufferSource();
                silentSource.buffer = buffer;
                silentSource.loop = true;
                
                // Create gain node set to almost silent
                const gainNode = this.audioContext.createGain();
                gainNode.gain.value = 0.001; // Very quiet
                
                // Connect nodes
                silentSource.connect(gainNode);
                gainNode.connect(this.audioContext.destination);
                
                // Start the silent audio
                silentSource.start();
                
                log('üîá Silent audio keepalive started');
            }
            
            startMonitoring() {
                this.monitorInterval = setInterval(() => {
                    if (this.audioContext && this.audioContext.state === 'suspended') {
                        log('‚ö†Ô∏è AudioContext suspended, attempting resume...');
                        this.audioContext.resume().then(() => {
                            log('‚úÖ AudioContext resumed automatically');
                        }).catch(err => {
                            log(`‚ùå Failed to resume: ${err.message}`);
                        });
                    }
                }, 2000); // Check every 2 seconds
            }
            
            cleanup() {
                if (this.monitorInterval) {
                    clearInterval(this.monitorInterval);
                }
                if (this.silentAudio) {
                    this.silentAudio.pause();
                }
            }
        }
        
        // üé§ VAD MANAGER
        async function initializeVAD() {
            try {
                log('üß† Loading VAD model...');
                
                // Wait for library to load
                let attempts = 0;
                while (!window.vad && attempts < 10) {
                    log(`Waiting for VAD library... attempt ${attempts + 1}`);
                    await new Promise(resolve => setTimeout(resolve, 500));
                    attempts++;
                }
                
                if (!window.vad || !window.vad.MicVAD) {
                    throw new Error('VAD library not found');
                }
                
                // Create VAD instance for manual audio processing
                if (window.vad.NonRealTimeVAD) {
                    vad = await window.vad.NonRealTimeVAD.new();
                    log('‚úÖ NonRealTimeVAD loaded and ready');
                } else {
                    // Fallback
                    vad = await window.vad.MicVAD.new({
                        onSpeechStart: () => log('VAD: Speech start'),
                        onSpeechEnd: () => log('VAD: Speech end'),
                        onVADMisfire: () => log('VAD: Misfire'),
                    });
                    log('‚úÖ MicVAD loaded as fallback');
                }
                
                vadReady = true;
                updateStatus('VAD Ready', 'vad-ready');
                return true;
            } catch (error) {
                log(`‚ùå VAD initialization failed: ${error.message}`);
                return false;
            }
        }
        
        // Initialize the keepalive manager
        const audioKeepAlive = new AudioContextKeepAlive();
        
        function log(message) {
            const logDiv = document.getElementById('log');
            const timestamp = new Date().toLocaleTimeString();
            logDiv.innerHTML += `[${timestamp}] ${message}\n`;
            logDiv.scrollTop = logDiv.scrollHeight;
        }
        
        function updateStatus(text, className) {
            const statusDiv = document.getElementById('status');
            statusDiv.textContent = text;
            statusDiv.className = `status ${className}`;
        }
        
        async function connect() {
            try {
                // üéØ ACTIVATE AUDIO CONTEXT KEEPALIVE ON USER GESTURE
                await audioKeepAlive.initializeOnUserGesture();
                
                // üß† INITIALIZE VAD
                await initializeVAD();
                
                ws = new WebSocket('ws://localhost:3000/api/audio/realtime');
                
                ws.onopen = () => {
                    log('‚úÖ Connected to WebSocket');
                    updateStatus('Connected', 'connected');
                    document.getElementById('connectBtn').disabled = true;
                    document.getElementById('disconnectBtn').disabled = false;
                    document.getElementById('recordBtn').disabled = false;
                };
                
                ws.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        log(`üì® Received: ${data.type}`);
                        
                        switch (data.type) {
                            case 'session_ready':
                                log('üéØ Session ready');
                                break;
                            case 'audio_delta':
                                // Play audio chunk (base64 encoded)
                                playAudioChunk(data.audio);
                                break;
                            case 'transcript_delta':
                                // Update live transcript
                                updateTranscript(data.text);
                                break;
                            case 'speech_started':
                                log('üé§ Speech detected');
                                updateStatus('Listening...', 'recording');
                                break;
                            case 'speech_stopped':
                                log('üîá Speech ended');
                                updateStatus('Connected', 'connected');
                                break;
                            case 'error':
                                log(`‚ùå Error: ${JSON.stringify(data.error)}`);
                                break;
                        }
                    } catch (e) {
                        log(`‚ùå Failed to parse message: ${e.message}`);
                    }
                };
                
                ws.onclose = () => {
                    log('üîå WebSocket closed');
                    updateStatus('Disconnected', 'disconnected');
                    resetButtons();
                };
                
                ws.onerror = (error) => {
                    log(`‚ùå WebSocket error: ${error}`);
                    updateStatus('Error', 'disconnected');
                    resetButtons();
                };
                
            } catch (error) {
                log(`‚ùå Connection failed: ${error.message}`);
            }
        }
        
        function disconnect() {
            if (ws) {
                ws.close();
            }
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                stopRecording();
            }
            audioKeepAlive.cleanup();
        }
        
        function resetButtons() {
            document.getElementById('connectBtn').disabled = false;
            document.getElementById('disconnectBtn').disabled = true;
            document.getElementById('recordBtn').disabled = true;
            document.getElementById('stopBtn').disabled = true;
        }
        
        async function startRecording() {
            try {
                // Get microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 16000, // VAD works best with 16kHz
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                
                // Use the keepalive AudioContext or create new one
                if (!audioContext && audioKeepAlive.audioContext) {
                    audioContext = audioKeepAlive.audioContext;
                } else if (!audioContext) {
                    audioContext = new AudioContext({ sampleRate: 16000 });
                }
                
                const source = audioContext.createMediaStreamSource(stream);
                
                // Create audio processor for VAD
                const processor = audioContext.createScriptProcessor(4096, 1, 1);
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                processor.onaudioprocess = async (event) => {
                    if (!vadReady || !vad) return;
                    
                    const inputBuffer = event.inputBuffer;
                    const audioData = inputBuffer.getChannelData(0);
                    
                    // üé§ VAD CHECK - Only send if speech detected
                    let speechProbability = 0;
                    if (vad && typeof vad.processPCMFloat32 === 'function') {
                        const vadResult = await vad.processPCMFloat32(audioData);
                        speechProbability = vadResult.isSpeech ? 1.0 : 0.0;
                    } else if (vad && typeof vad.process === 'function') {
                        speechProbability = await vad.process(audioData);
                    }
                    const isCurrentlySpeaking = speechProbability > 0.5; // Threshold
                    
                    if (isCurrentlySpeaking !== isSpeaking) {
                        isSpeaking = isCurrentlySpeaking;
                        if (isSpeaking) {
                            log('üó£Ô∏è Speech detected (VAD)');
                            updateStatus('Speaking...', 'speaking');
                        } else {
                            log('ü§´ Speech ended (VAD)');
                            updateStatus('Recording...', 'recording');
                        }
                    }
                    
                    // Only send audio when VAD detects speech
                    if (isSpeaking && ws && ws.readyState === WebSocket.OPEN) {
                        // Convert to base64 and send
                        const audioArray = new Float32Array(audioData);
                        const buffer = new ArrayBuffer(audioArray.length * 2);
                        const view = new DataView(buffer);
                        
                        // Convert to 16-bit PCM
                        for (let i = 0; i < audioArray.length; i++) {
                            const sample = Math.max(-1, Math.min(1, audioArray[i]));
                            view.setInt16(i * 2, sample * 0x7FFF, true);
                        }
                        
                        const base64 = btoa(String.fromCharCode(...new Uint8Array(buffer)));
                        
                        ws.send(JSON.stringify({
                            type: 'audio_chunk',
                            audio: base64
                        }));
                    }
                };
                
                log('üé§ Recording started with VAD');
                updateStatus('Recording...', 'recording');
                document.getElementById('recordBtn').disabled = true;
                document.getElementById('stopBtn').disabled = false;
                
            } catch (error) {
                log(`‚ùå Recording failed: ${error.message}`);
            }
        }
        
        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
            }
            
            // Reset VAD state
            isSpeaking = false;
            
            log('üõë Recording stopped');
            updateStatus('Connected', 'connected');
            document.getElementById('recordBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;
        }
        
        function updateTranscript(text) {
            const transcriptDiv = document.getElementById('transcript');
            transcriptDiv.textContent += text;
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }
        
        function playAudioChunk(base64Audio) {
            if (!audioContextReady || !audioKeepAlive.audioContext) {
                log('‚ö†Ô∏è AudioContext not ready, audio playback skipped');
                return;
            }
            
            try {
                // üéØ PROPER AUDIO PLAYBACK USING WEB AUDIO API
                const audioData = atob(base64Audio);
                const arrayBuffer = new ArrayBuffer(audioData.length);
                const view = new Uint8Array(arrayBuffer);
                for (let i = 0; i < audioData.length; i++) {
                    view[i] = audioData.charCodeAt(i);
                }
                
                // Decode audio data and play through AudioContext
                audioKeepAlive.audioContext.decodeAudioData(arrayBuffer.slice()).then(audioBuffer => {
                    const source = audioKeepAlive.audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioKeepAlive.audioContext.destination);
                    source.start();
                    log(`üîä Playing audio chunk (${audioBuffer.duration.toFixed(2)}s)`);
                }).catch(error => {
                    // Fallback to regular audio element if decoding fails
                    log(`‚ö†Ô∏è AudioContext decode failed, using fallback: ${error.message}`);
                    const audio = new Audio('data:audio/wav;base64,' + base64Audio);
                    audio.play().catch(err => {
                        log(`‚ùå Fallback audio failed: ${err.message}`);
                    });
                });
                
            } catch (error) {
                log(`‚ùå Audio playback error: ${error.message}`);
            }
        }
        
        // Auto-connect on page load for testing
        window.onload = () => {
            log('üöÄ Page loaded, ready to connect');
            log('üí° Click "Connect" to activate audio system and load VAD');
        };
    </script>
</body>
</html>